---
layout: post
category: 深度学习算法
tags: [深度学习算法,LSTM]
---

长短期记忆网络 (Long Short Term)
===============

## 门控RNN

### 长短期记忆 （LSTM）

> 一种特殊的 RNN。传统RNN有两个问题：
  1. 梯度爆炸 （用cliping 解决）
  2. 梯度消失，以及梯度消失导致的“长期依赖问题”。 （用LSTM解决）

### 门控循环单元 （GRU, gated recurrent unit）

## LSTM, Long short Term

> The repeating module in an LSTM contains four interacting layer

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-chain.png){:width='250px' height="100px"}

  每个元素的涵义：
	
![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM2-notation.png){:width='500px' height="100px"}

  LSTM之所以能够记住长期的信息，在于设计的“门”结构，“门”结构是一种让信息选择式通过的方法，包括一个sigmoid神经网络层和一个pointwise乘法操作。sigmoid layer 输出0到1之间的数字，0表示“let nothing through”，1表示“let everything through!”。

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-gate.png){:width='100px' height="125px"}

  在LSTM中，第一阶段是遗忘门，遗忘层决定哪些信息需要从细胞状态中被遗忘，下一阶段是信息增加门，输入门确定哪些新信息能够被存放到细胞状态中，最后一个阶段是输出门，输出门确定输出什么值。

### 遗忘门 (forget gate layer, decide what information we’re going to throw away from the cell state)

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-f.png){:width='300px' height="100px"}

### 信息增加门 (decide what new information we’re going to store in the cell state).

step1 : input gate layer (decides which values we’ll update.),sigmoid决定了什么值需要更新.
step2 : tanh创建一个新的细胞状态的候选向量Ct，该过程训练两个权值Wi和Wc。

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-i.png){:width='300px' height="100px"}

step3: we’ll combine these two to create an update to the state.

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-C.png){:width='300px' height="100px"}

### 输出门(decide what we’re going to output)

step1: we run a sigmoid layer which decides what parts of the cell state we’re going to output.
step2: put the cell state through tanh
 (to push the values to be between −1and 1) and multiply it by the output of the sigmoid gate

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-o.png){:width='300px' height="100px"}

### 实现

#### BasicLSTMCell

before [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py)

	tf.compat.v1.nn.rnn_cell.BasicLSTMCell
	
Tf2.0 [documentation](https://keras.io/layers/recurrent/) [code](https://github.com/tensorflow/tensorflow/blob/d90e521d71b88f469e68eb1a467606ea6d44c733/tensorflow/python/keras/layers/recurrent.py)

	tf.compat.v1.nn.rnn_cell.LSTMCell
	
	class LSTMCell(DropoutRNNCellMixin, Layer):
		def __init__(self,
			     units,
			     activation='tanh',
               		     recurrent_activation='hard_sigmoid',
               		     use_bias=True,
			     ...
			     )
			super(LSTMCell, self).__init__(**kwargs)
			...
			self.state_size = data_structures.NoDependency([self.units, self.units])
    			self.output_size = self.units
			
	
		def build(self, input_shape):
			'''
			初始化 self.kernel, self.recurrent_kernel, self.bias
			'''
			...

		def call()
			h_tm1 = states[0]  # previous memory state
    			c_tm1 = states[1]  # previous carry state
			...
			为每一步创建bias
			x = (x_i, x_f, x_c, x_o)
			为每一步创建初始状态
      			h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)
			...
			# 遗忘门
			f = self.recurrent_activation(x_f + K.dot(
       	 			h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
			# 信息增加门
			i = self.recurrent_activation(
        			x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))
			c = f * c_tm1 + i * self.activation(x_c + K.dot(
        			h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))			

			# 输出门
			o = self.recurrent_activation(
        			x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))
			h = o * self.activation(c)
    			return h, [h, c]
	
		def get_config(self):
			...
			base_config = super(LSTMCell, self).get_config()
    			return dict(list(base_config.items()) + list(config.items()))

## 参考资料


[1] 伊恩古德费洛.深度学习[M].北京:人民邮电出版社.2017:34-51
[2] Understanding LSTM Networks [html](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
[3] Understanding LSTM in Tensorflow(MNIST dataset) [html](http://1t.click/5ZK)
[4] BasicLSTMCell中num_units参数解释 [html](https://blog.csdn.net/notHeadache/article/details/81164264)
