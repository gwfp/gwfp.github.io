---
layout: post
category: 深度学习算法
tags: [深度学习算法,LSTM]
---

长短期记忆网络 (Long Short Term)
===============

## 门控RNN

### 长短期记忆 （LSTM）

> 一种特殊的 RNN。传统RNN有两个问题：
  1. 梯度爆炸 （用cliping 解决）
  2. 梯度消失，以及梯度消失导致的“长期依赖问题”。 （用LSTM解决）

### 门控循环单元 （GRU, gated recurrent unit）

> LSTM最流行的一个变体。

## LSTM, Long short Term

> The repeating module in an LSTM contains four interacting layer

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-chain.png){:width='250px' height="100px"}

  每个元素的涵义：
	
![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM2-notation.png){:width='500px' height="100px"}

  LSTM之所以能够记住长期的信息，在于设计的“门”结构，“门”结构是一种让信息选择式通过的方法，包括一个sigmoid神经网络层和一个pointwise乘法操作。sigmoid layer 输出0到1之间的数字，0表示“let nothing through”，1表示“let everything through!”。

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-gate.png){:width='100px' height="125px"}

  在LSTM中，第一阶段是遗忘门，遗忘层决定哪些信息需要从细胞状态中被遗忘，下一阶段是信息增加门，输入门确定哪些新信息能够被存放到细胞状态中，最后一个阶段是输出门，输出门确定输出什么值。

### 遗忘门 (forget gate layer, decide what information we’re going to throw away from the cell state)

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-f.png){:width='300px' height="100px"}

### 信息增加门 (decide what new information we’re going to store in the cell state).

step1 : 输入门（input gate layer ，decides which values we’ll update.),sigmoid决定了什么值需要更新.
step2 : tanh创建一个新的细胞状态的候选向量Ct，该过程训练两个权值Wi和Wc。

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-i.png){:width='300px' height="100px"}

step3: we’ll combine these two to create an update to the state.

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-C.png){:width='300px' height="100px"}

### 输出门(output gate，decide what we’re going to output)

step1: we run a sigmoid layer which decides what parts of the cell state we’re going to output.
step2: put the cell state through tanh
 (to push the values to be between −1and 1) and multiply it by the output of the sigmoid gate

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-focus-o.png){:width='300px' height="100px"}

### 实现

#### BasicLSTMCell

before [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py)

	tf.compat.v1.nn.rnn_cell.BasicLSTMCell
	
Tf2.0 [documentation](https://keras.io/layers/recurrent/) [code](https://github.com/tensorflow/tensorflow/blob/d90e521d71b88f469e68eb1a467606ea6d44c733/tensorflow/python/keras/layers/recurrent.py)

	tf.compat.v1.nn.rnn_cell.LSTMCell
	
	class LSTMCell(DropoutRNNCellMixin, Layer):
		def __init__(self,
			     units,	# LSTM输出结果的维度
			     activation='tanh',
               		     recurrent_activation='hard_sigmoid',
               		     use_bias=True,
			     ...
			     )
			super(LSTMCell, self).__init__(**kwargs)
			...
			self.state_size = data_structures.NoDependency([self.units, self.units])
    			self.output_size = self.units
			
	
		def build(self, input_shape):
			'''
			初始化 self.kernel, self.recurrent_kernel, self.bias
			'''
			...

		def call(self, inputs, states, training=None)
			h_tm1 = states[0]  # previous memory state
    			c_tm1 = states[1]  # previous carry state
			...
			为每一步创建bias
			x = (x_i, x_f, x_c, x_o)
			为每一步创建初始状态
      			h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)
			...
			# 遗忘门
			f = self.recurrent_activation(x_f + K.dot(
       	 			h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))
			# 信息增加门
			i = self.recurrent_activation(
        			x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))
			c = f * c_tm1 + i * self.activation(x_c + K.dot(
        			h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))			

			# 输出门
			o = self.recurrent_activation(
        			x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))
			
			h = o * self.activation(c)
			# h,c 对应下一个cell的输入状态 state_next = [h,c]
    			return h, [h, c]
	
		def get_config(self):
			...
			base_config = super(LSTMCell, self).get_config()
    			return dict(list(base_config.items()) + list(config.items()))

	
相关解释：
	num_units： 个参数的大小就是LSTM输出结果的维度。例如num_units=128， 那么LSTM网络最后输出就是一个128维的向量。[html](https://blog.csdn.net/notHeadache/article/details/81164264)
	call（）函数return h，和 [h, c]（隐藏层状态），如果我们处理的是分类问题，那么我们还需要对h添加单独的Softmax层才能得到最后的分类概率输出。

## GRU

> GRU包括两个门，一个重置门(rt, reset gate)和更新门(zt, update gate)。这两个门的激活函数为sigmoid函数，在[0,1]区间取值。

![avatar](https://gwfp.github.io/static/images/19/07/01/LSTM3-var-GRU.png){:width='300px' height="100px"}

### 重置门

> 决定过去有多少信息被遗忘，有助于捕捉时序数据中短期的依赖关系。

### 更新门

> 有助于捕捉时序数据中中长期的依赖关系。

### 实现

#### GRUCell

before： [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py)

	tf.compat.v1.nn.rnn_cell.GRUCell

Tf2.0:  [code](https://github.com/tensorflow/tensorflow/blob/d90e521d71b88f469e68eb1a467606ea6d44c733/tensorflow/python/keras/layers/recurrent.py)

	tf.keras.layers.GRUCell

	class GRUCell(DropoutRNNCellMixin, Layer):
		def __init__(self,
               		     units,
               		     activation='tanh',
               		     recurrent_activation='hard_sigmoid',
               		     use_bias=True,
			     ...)
			...
			self.state_size = self.units
    			self.output_size = self.units

		def build(self, input_shape):
			input_dim = input_shape[-1]
			'''
			初始化 self.kernel， self.recurrent_kernel， self.use_bias
			'''
		        ...
			self.built = True
		
		def call(self, inputs, states, training=None):
			h_tm1 = states[0]  # previous memory
			
			x_z = K.dot(inputs_z, self.kernel[:, :self.units])
      			x_r = K.dot(inputs_r, self.kernel[:, self.units:self.units * 2])
      			x_h = K.dot(inputs_h, self.kernel[:, self.units * 2:])

			recurrent_z = K.dot(h_tm1, self.recurrent_kernel[:, :self.units])
      			recurrent_r = K.dot(h_tm1,
                          self.recurrent_kernel[:, self.units:self.units * 2])
			
			# update gate
			z = self.recurrent_activation(x_z + recurrent_z)
			# reset gate
			r = self.recurrent_activation(x_r + recurrent_r)
			# hidden state
			recurrent_h = K.dot(r * h_tm1,
                            self.recurrent_kernel[:, 2 * self.units:])
			hh = self.activation(x_h + recurrent_h)
			# previous and candidate state mixed by update gate
    			h = z * h_tm1 + (1 - z) * hh
			return h, [h]
			     
		def get_config(self):
			config = {'units': self.units,
        			'activation': 
					activations.serialize(self.activation),
        			'recurrent_activation':
            				activations.serialize(self.recurrent_activation),
				...}
			base_config = super(GRUCell, self).get_config()
			return dict(list(base_config.items()) + list(config.items()))
		
		def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
			return _generate_zero_filled_state_for_cell(self, inputs, batch_size, dtype)		


## 参考资料


[1] 伊恩古德费洛.深度学习[M].北京:人民邮电出版社.2017:34-51
[2] Understanding LSTM Networks [html](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
[3] Understanding LSTM in Tensorflow(MNIST dataset) [html](http://1t.click/5ZK)
