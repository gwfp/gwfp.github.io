---
layout: post
category: 深度学习算法
tags: [深度学习概念,RNN,循环神经网络]
---

循环神经网络(recurrent neural network,RNN)
===============

## RNN的作用

> RNN 是一类专门用于处理序列数据的神经网络。可以扩展到更长的序列，也能处理可变长度的序列。

## RNN的模型

### RNNCell

> RNN 都具有一种重复神经网络模块的链式的形式,在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。

![avatar](https://gwfp.github.io/static/images/19/06/19/RNN.png){:width='450px' height="180px"}

> 每个RNNCell 都有一个 call 方法，使用方式：

	(output, next_state) = call(input, state)
  
  假设我们有一个初始状态h0，还有输入x1，调用call(x1,h0)后就可以得到(output1, h1)：

![avatar](https://gwfp.github.io/static/images/19/06/19/RNNcallstep1.png){:width='350px' height="150px"}
 
  再调用一次call(x2, h1)就可以得到(output2, h2)：

![avatar](https://gwfp.github.io/static/images/19/06/19/RNNcallstep2.png){:width='350px' height="130px"}

  每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”。代码实现上，RNNCell只是一个抽象类，我们用的时候都是用的它的两个子类BasicRNNCell和BasicLSTMCell。 
 
$$
	h_{t} = tanh(W_{hh}h_{t-1}+W_{xh}h_{t})
$$

	def call(self, x):
		self.h = np.tanh(np.dot(self.w_hh, self.h) + np.dot(self.W_xh, x))
		y = np.dot(self.W_hy, self.h)
		return y

#### BasicRNNCell

> RNN的基础类

before [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py) ,(base_layer) [code](https://github.com/tensorflow/tensorflow/blob/6b4b4d417db40595d3593802c560420591c3f2bc/tensorflow/python/keras/engine/base_layer.py)

	tf.compat.v1.nn.rnn_cell.BasicRNNCell

	class BasicRNNCell(LayerRNNCell):

		def __init__(self,
			num_units,	#int, The number of units in the RNN cell.
               		activation=None, #Default: `tanh`
               		reuse=None,	#
               		name=None,	#String, when  layer name is same, reuse = True 
               		dtype=None, 	
               		**kwargs)
		...

		def state_size(self):
    			return self._num_units 	# 隐层的大小

		def output_size(self):
    			return self._num_units  # 输出的大小
	'''
		我们通常是将一个batch送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。
	'''

		def build(self, inputs_shape):
			... 
			self._kernel = add_variable('kernel'
				, shape=[input_shape[-1] + self._num_units
				, self._num_units])	#num_units: int, The number of units in the RNN cell.
			self._bias = add_variable('bias'
				, shape=[self._num_units],
				, initializer=init_ops.zeros_initializer(dtype=self.dtype))
			self.built=True
		

		def call(self, inputs, state)
			# gate_inputs = W * (input + state)
			gate_inputs = math_ops.matmul(
        			array_ops.concat([inputs, state], 1), self._kernel)
			# gate_inputs = gete_input + bias
			gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) 
			output = self._activation(gate_inputs) 
			return output, output

		def get_config(self):
			config = {
				...
			}
			base_config = super(BasicRNNCell, self).get_config()
			# gate_inputs = W * (input + state)
			return dict(list(base_config.items()) + list(config.items()))

Tf2.0 [code](https://github.com/tensorflow/tensorflow/blob/d90e521d71b88f469e68eb1a467606ea6d44c733/tensorflow/python/keras/layers/recurrent.py)

	tf.keras.layers.SimpleRNNCell

	

#### BasicLSTMCell 

> [LSTM](https://gwfp.github.io/深度学习算法/2019/07/01/LSTM.html) 的基础类

### 一次执行多步RNN

#### dynamic_rnn

	# inputs: shape = (batch_size, time_steps, input_size) ,time_steps表示序列本身的长度
	# cell: RNNCell
	# initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取零矩阵
	outputs, state = tf.compat.v1.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)

#### static_bidirectional_rnn

### 堆叠RNNCell

#### before [code]()
		
	# 对RNNCell进行堆叠
	tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers, state_is_tuple=True)
	# 对MultiRNNCell进行初始化
	init_state = cell.zero_state(batch_size, tf.float32)

#### Tf2.0  [code]()

	tf.keras.layers.StackedRNNCells
	‘’‘
	   Wrapper allowing a stack of RNN cells to behave as a single cell.

	   Example：
		cells = [
   		   keras.layers.LSTMCell(output_dim),
      	   	   keras.layers.LSTMCell(output_dim),
      		   keras.layers.LSTMCell(output_dim),
  		]
	’‘’
	...
	def call(self, inputs, states, constants=None, **kwargs):
		 # Recover per-cell states.
    		 state_size = (self.state_size[::-1]
                 if self.reverse_state_order else self.state_size)
    nested_states = nest.pack_sequence_as(state_size, nest.flatten(states))

    		 # Call the cells in order and store the returned states.
    		 new_nested_states = []
    		 for cell, states in zip(self.cells, nested_states):
      			states = states if nest.is_sequence(states) else [states]
      			# TF cell does not wrap the state into list when there is only one state.
      			is_tf_rnn_cell = getattr(cell, '_is_tf_rnn_cell', None) is not None
      			states = states[0] if len(states) == 1 and is_tf_rnn_cell else states
      			if generic_utils.has_arg(cell.call, 'constants'):
        			inputs, states = cell.call(inputs, states, constants=constants,**kwargs)
      			else:
        			inputs, states = cell.call(inputs, states, **kwargs)
      			new_nested_states.append(states)

    		 return inputs, nest.pack_sequence_as(state_size,
                                nest.flatten(new_nested_states))


## RNN的种类

![avatar](https://gwfp.github.io/static/images/19/06/19/kindofRNN.jpeg){:width='300px' height="100px"}

### many to one

### one to one

### one to many

### many(1) to many(2)  # (1) 不一定等于 (2) 


## 参考资料


[1] 伊恩古德费洛.深度学习[M].北京:人民邮电出版社.2017:34-51
[2] TensorFlow中RNN实现的正确打开方式[html] (https://zhuanlan.zhihu.com/p/28196873)
[3] LSTM（长短期记忆网络）及其tensorflow代码应用 [html](https://www.cnblogs.com/pinking/p/9362966.html)
[4] 长短期记忆网络（Long Short-Term Memory，LSTM）及其变体双向LSTM和GRU [html](https://blog.csdn.net/weixin_42111770/article/details/80900575)
[5] RNNCell实现 [code](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/rnn_cell_impl.py)

