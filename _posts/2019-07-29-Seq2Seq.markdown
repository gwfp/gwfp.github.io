---
layout: post
category: 深度学习模型
tags: [深度学习模型,Seq2Seq.注意力机制,Attention Model]
---

Seq2Seq (Sequence-to-sequence)
===============

## 概念

### Seq2Seq

> Seq2seq,是一种通用的编码器——解码器框架，可用于机器翻译、文本摘要、会话建模、图像字幕等场景中。包含两部分：Encoder和Decoder

![avatar](https://gwfp.github.io/static/images/19/07/29/seq2seq.png){:width='600px' height="200px"}

### Attention Model

> 为了解决,seq2seq在句子过长时效果不佳的问题，引入注意力机制。

## RNN Encoder-Decoder



## 参考资料


[1] Dzmitry Bahdanau.Neural Machine Translation by jointly Learning to Align and Translate[J].ICLR,2015. 
[2] Sequence to Sequence Learning with Neural Networks [pdf](https://arxiv.org/pdf/1409.3215.pdf)
[3] Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation [pdf](https://arxiv.org/pdf/1406.1078.pdf)
[4] legacy_seq2seq [Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py)
[5] seq2seq [code](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops)
[6] Neural Machine Translation (seq2seq) Tutorial [link](https://github.com/tensorflow/nmt)
[7] Neural Machine Translation with Attention [link](https://tensorflow.google.cn/beta/tutorials/text/nmt_with_attention)
[8] W3Cschool Tensorflow 官方文档 [html](https://www.w3cschool.cn/tensorflow_python/tensorflow_python-bm7y28si.html)
[9] attention [code](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.6%20Attention)
[10] 深度学习项目实战计划 [html](https://blog.csdn.net/chinatelecom08/article/details/83388246)

