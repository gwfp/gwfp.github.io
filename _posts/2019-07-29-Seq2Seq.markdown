---
layout: post
category: 深度学习模型
tags: [深度学习模型,Seq2Seq.注意力机制,Attention Model]
---

Seq2Seq (Sequence-to-sequence)
===============

## 概念

### Seq2Seq

> Seq2seq,是一种通用的编码器——解码器框架，可用于机器翻译、文本摘要、会话建模、图像字幕等场景中。包含两部分：Encoder和Decoder

![avatar](https://gwfp.github.io/static/images/19/07/29/seq2seqarchitectrue.png){:width='600px' height="200px"}

### Attention Model

> 为了解决,seq2seq在句子过长时效果不佳的问题，引入注意力机制。

## RNN Encoder-Decoder



## 参考资料


[1] Dzmitry Bahdanau.Neural Machine Translation by jointly Learning to Align and Translate[J].ICLR,2015. 
[2] [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
[3] [Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)
[4] [legacy_seq2seq Code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py)
[5] [seq2seq](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops)
