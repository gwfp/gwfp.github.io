---
layout: post
category: 机器学习概念
tags: [机器学习]
---


机器学习 Machine Learning
================

## 机器学习的基本原理

### 基于规则(Rule-Based) vs 基于模型(Model-Based)

## 机器学习的基本方式

### 有监督学习 (Supervised Learning)

> 1. 训练数据同时拥有输入变量（X）和输出变量（y） \\
  2. 用一个算法把输入和输出的映射关系 （ y=f(x) ）学习出来。 \\
  3. 当我们拿到新数据x‘ 以后 就可以通过已经被学习出的f(.),得到相应的y‘。

### 无监督学习 (Unsupervised Learning)

> 1. 训练数据只有输入变量(x), 没有输出变量。 \\
  2. 无监督学习的目的是将这些训练数据潜在的结构或者分布找出来，以便我们对这些数据有更多的了解。

### 半监督学习 （Semi-supervised Learning）

> 有一部分训练数据的输入变量(x)有对应的输出变量（y）,另一些则没有。

> 1. 用无监督学习技术来发现和学习输入变量的结构。
  2. 用有监督学习对未标注数据的输出结果进行“猜测”。
  3. 将带着猜测标签的数据作为训练数据，训练有监督模型。

### 强化学习(reinforcement)

> 在未知采取何种行为的情况下，学习者必须通过不断尝试才能发现采取哪种行为能够产生最大回报。

### 集成学习(Ensemble Learning)

> 采用多个分类器对数据集进行预测，从而提高整体分类器的泛化能力.

### 神经网络和深度学习(Neural Networks and deep learning)

> 待添加

## 机器学习解决的问题

### 分类问题(classification)

### 回归问题(regression)

### 结构化问题(structured)

## 机器学习三要素：数据、模型、算法

### 数据

#### 源数据 (Raw data)

> 现实中的数据的样本集合

#### 无标注数据
	
> 向量空间模型(Vector Space Model/VSM): 将各种格式(文字、图片、音频、视频)转化为一个个向量 

> 特征向量(Feature Vector): 

#### 有标注数据

> 给训练样本打上标签，每一个标注样本即有无标注样本拥有的X，同时还比无标注样本多一个y。

> [特征工程](https://gwfp.github.io/机器学习概念/2019/02/16/FeatureEngineering.html)：1.确定用哪些特征来表示数据；2.确定用过什么方式表达这些特征；

### 模型

> 数据 + 算法 => 模型

> 训练：通过 数据 和 算法 获得模型的过程  

### 算法

> 损失函数(Loss Function): L(y,y')=L(y,f(x)),  描述y与y'之间的差别

> 代价函数(Cost Function): J(theta), 整个模型付出的代价

#### 常用的机器学习算法（部分）

.|Unsupervised|Supervised|
-|-|-
Continuous|Clustering&Dimensionality,<br>Reduction(SVD,PCA,K-means)|Regression(Linear,Polynomial),<br>Decision Tree,<br>Random Forests
Categorical|Association Analysis(Aproori,FP-Growth),<br>Hidden Markov Model | Classification(KNN,Trees,Logistic Regression,Naive-Bayes,SVM)


## 获取模型的过程

### setp 1: 数据准备。

#### setp 1.1 数据预处理: 收集数据、清洗数据、标注数据

#### setp 1.2 构建数据的向量空间模型（将文本、图片、音频、视频等格式的数据转换为向量）。

#### setp 1.3 将构建好向量空间的模型的数据分为训练集、验证集和测试集

> 训练集 (Train Set) : 用来做训练的集合

> 验证集（Validation Set): 用来在训练过程中每个训练轮次结束后验证当前模型性能，为进一步优化模型提供参考的数据集合。

> 测试集 (Test Set) : 用来测试的数据集合，用于检验最终得出的模型性能。

### setp 2: 训练 

> 将测试集输入给训练集程序，进行运算。训练程序的核心是算法，所有输入的向量话数据都会按该训练程序所依据的算法进行运算。训练程序输出的结果就是模型。

#### setp 2.1: 编写训练程序

##### setp 2.2.1: 选择模型类型

##### setp 2.2.2: 选择优化算法

##### setp 2.2.3: 根据模型类型和算法编写程序

#### setp 2.2: 训练 -> 获得模型

#### setp 2.3: 在训练集上运行模型，获得训练集预测结果。

#### setp 2.4: 在验证集上运行临时模型，获得验证集预测结果。

#### setp 2.5: 综合参照 step 2.4 和 step 2.5 的预测结果，改进模型

#### setp 2.6: step 2.2 到 step 2.5 反复迭代，直到获得让我们满意，或者已经无法继续优化的模型。

### setp 3: 测试

> 将测试集输入给训练获得的模型，得到预测结果，再将预测结果与这些数据原本的预期结果做比较。

## 改进模型

> 根据setp 2.5进行改进

### 数据

> 机器学习的模型质量往往和训练数据有直接关系，大量高质量训练数据，是提高模型质量的有效手段。

### 有限数据上，如何尽量提高质量

> 对数据进行归一化,(Normalization) , 正则化（Regularization）等标准化操作。 \\
  采用 Bootstrap 等采样方法处理有限训练/测试数据，以达到更好的运算效果。  \\
  根据业务进行特征提取：从业务角度区分输入数据包含的特征，并理解这些特征对结果的贡献。
  
## 调参

> 超参数

### 步骤

#### 1.制定目标

#### 2.制定策略

#### 3.执行

#### 4.验证

#### 5.调整策略 -> 3

## 衡量模型的质量

### 分类模型的评判指标

> 当一个数据集被预测完之后，假设data1 被模型预测的类别为class_A 

实际/预测	|。预测数据为class_A  |  预测为其他类 |
实际为class_A。 |: TP  :|: FN :|
实际为其他类。  |: FP  :|: TN :|

> TP : 实际为class_A,也被正确预测的条数。 \\
  FN : 实际为class_A,但被预测为其他类的测试数据集条数。 \\
  FP : 实际不为class_A,但被预测为class_A的数据条数。 \\
  TN : 实际不为class_A,也没有被预测为class_A的数据条数。  

#### 精准率(Precision)

> Precision = TP / (TP + FP)

#### 召回率（Recall）

> Recall = TP / (TP + FN)

#### F1Score

> F1Score = 2 * (Precision * Recall) / Precision + Recall

	1.虽然上面三个值都是越大越好，但往往在实际中 P 和 R 是矛盾的，很难达到双高。
	2.P，R，F1score 都是对某一个类而言的，对数据而言，每个类都有独立的P，R，F1Score 值。


### 模型的偏差和过拟合

#### 欠拟合(Underfitting)

#### [过拟合(Overfitting)](https://gwfp.github.io/机器学习概念/2019/01/22/Overfitting.html)

> 过拟合：算法所训练的数据模型过多地表达了数据间的噪音关系。


### 学习曲线

> 随着训练样本增多，算法训练出的模型的表现能力。

### 交叉验证 Cross Validation

### 偏差方差权衡 Bias Variance Trade off

> 模型误差 = 偏差(Bias) + 方差(Variance) + 不可避免的误差

> 导致方差的主要原因：对问题本身的假设不正确。（欠拟合）

> 导致偏差的主要原因: 使用的模型太复杂。（过拟合）

> 1. 非参数学习算法通常都是高方差算法。 \\
  2. 参数学习算法通常都是高偏差算法。 	\\
  3. 大多数算法具有相应的参数，可以调整偏差和方差 \\
  4. 偏差和方差通常是矛盾的，降低偏差会提高方差；降低方差会提高偏差；\\

> 解决高方差的通常手段：	\\
  1. 降低模型复杂度。
  2. 减少数据维度，降噪		\\
  3. 增加样本数		\\
  4. 使用验证集		\\
  5. 模型的正则化

## 模型范化

### 模型正则化 Regularization

> 限制参数的大小
$$
L1 正则项: \alpha \frac{1}{2}\sum_{i=1}^{n}\theta _{i}^{2}	\
L2 正则项：\alpha \sum_{i=1}^{n}\left | \theta _{i} \right |
$$

> 岭回归 Ridge Regression ：使下式尽可能小	\
$$
	J(\theta )=MSE(y,\hat{y}|\theta )+\alpha \frac{1}{2}\sum_{i=1}^{n}\theta _{i}^{2}
$$

> LASSO Regression: 使下式尽可能小	\
$$
	J(\theta )=MSE(y,\hat{y}|\theta )+\alpha \sum_{i=1}^{n}\left | \theta _{i} \right |
$$

> 弹性网 Elastic Net ：		\
$$
	J(\theta )=MSE(y,\hat{y}|\theta )+r\alpha \sum_{i=1}^{n}\left | \theta _{i} \right | + \frac{1-r}{2}\alpha \sum_{i=1}^{n}\theta _{i}^{2}
$$

## 数据降维

### 维度灾难

> 当维度增加时，由于向量体积指数型增加，会遇到很多低维向量中很难出现的问题

### 数据稀疏

> 休斯现象：在训练样本固定的情况下，特征维数增加到某一个临界点后，继续增加反而会导致模型预测能力减小。
